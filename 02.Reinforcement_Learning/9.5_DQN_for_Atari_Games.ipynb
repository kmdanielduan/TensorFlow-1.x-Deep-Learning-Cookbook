{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5_DQN_for_Atari_Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.misc import imresize\n",
    "import time\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EXPERIENCES = 500000\n",
    "MIN_EXPERIENCES = 50000\n",
    "TARGET_UPDATE_PERIOD = 10000\n",
    "IM_SIZE = 80\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    img_temp = img[31:195] # choose the important area of the image\n",
    "    img_temp = img_temp.mean(axis=2) # Convert to Grayscale\n",
    "    \n",
    "    # Downsample image using nearest neighbour interpolation\n",
    "    img_temp = resize(img_temp, (IM_SIZE,IM_SIZE))\n",
    "    return img_temp\n",
    "\n",
    "# testing for the above function\n",
    "def test_preprocess():\n",
    "    env = gym.make(\"Breakout-v0\")\n",
    "    observation = env.reset()\n",
    "    img = preprocess(observation)\n",
    "\n",
    "    plt.imshow(observation)\n",
    "    plt.show()\n",
    "    print(observation.shape)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(img.shape)\n",
    "    env.close()\n",
    "\n",
    "# test_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(state, obs):\n",
    "    obs_small = preprocess(obs)\n",
    "    return np.append(state[1:], np.expand_dims(obs_small, 0), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, K, scope, save_path = 'models/atari.ckpt'):\n",
    "        self.K = K \n",
    "        self.scope = scope\n",
    "        self.save_path = save_path\n",
    "        \n",
    "#         tf.reset_default_graph()\n",
    "        with tf.variable_scope(scope):\n",
    "            \n",
    "            # inputs and targets\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, 4, IM_SIZE, IM_SIZE), name='X')\n",
    "            # tensorflow convolution needs the order to be:\n",
    "            # (num_samples, height, width, \"color\")\n",
    "            # so we need to transpose later\n",
    "            self.G = tf.placeholder(tf.float32, shape=(None,), name='G')\n",
    "            self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n",
    "            \n",
    "            # calculate output and cost\n",
    "            # convolutional layers\n",
    "            Z = self.X / 255.0\n",
    "            Z = tf.transpose(Z, [0,2,3,1])\n",
    "            # tf.contrib.layers.conv2d\n",
    "            # input:[batch_size] + input_spatial_shape + [in_channels]\n",
    "            # (num_outputs, kernel_size, stride=1)\n",
    "            cnn1 = tf.contrib.layers.conv2d(Z, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "            cnn2 = tf.contrib.layers.conv2d(cnn1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "            cnn3 = tf.contrib.layers.conv2d(cnn2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "            \n",
    "            # fully connected layers\n",
    "            fc0 = tf.contrib.layers.flatten(cnn3)\n",
    "            fc1 = tf.contrib.layers.fully_connected(fc0, 512)\n",
    "            \n",
    "            # final output layer\n",
    "            self.predict_op = tf.contrib.layers.fully_connected(fc1,K)\n",
    "            selected_action_values = tf.reduce_sum(\n",
    "                self.predict_op * tf.one_hot(self.actions, K),\n",
    "                reduction_indices = [1]\n",
    "            )\n",
    "            # tf.reduce_sum : get the sum along an axis\n",
    "            \n",
    "            self.cost = tf.reduce_mean(tf.square(self.G - selected_action_values))\n",
    "            self.train_op = tf.train.RMSPropOptimizer(learning_rate=0.00025, decay=0.9,\n",
    "                                                      momentum=0.0, epsilon=1e-6\n",
    "                                                     ).minimize(self.cost)\n",
    "    \n",
    "    def set_session(self,session):\n",
    "        self.session = session\n",
    "        \n",
    "    def predict(self, states):\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X:states})\n",
    "        \n",
    "    def update(self, states, actions, targets):\n",
    "        c, _ = self.session.run(\n",
    "            [self.cost, self.train_op],\n",
    "            feed_dict={\n",
    "                self.X:states,\n",
    "                self.G:targets,\n",
    "                self.actions:actions\n",
    "            }\n",
    "        )\n",
    "        return c\n",
    "    \n",
    "    def sample_action(self, x, eps):\n",
    "        \"\"\"implements epsilon greedy algorithm\"\"\"\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            return np.argmax(self.predict([x])[0])\n",
    "    \n",
    "    def load(self):\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        load_was_success = True\n",
    "        try:\n",
    "            save_dir = '/'.join(self.save_path.split('/')[:-1])\n",
    "            ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "            load_path = ckpt.model_checkpoint_path\n",
    "            self.saver.restore(self.session, load_path)\n",
    "        except:\n",
    "            print('no saved model to load. starting new session')\n",
    "            load_was_success = False\n",
    "        else:\n",
    "            print(\"loaded model: {}\".format(load_path))\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            episode_number = int(load_path.split('-')[-1])\n",
    "            \n",
    "    def save(self, n):\n",
    "        self.saver.save(self.session, self.save_path, global_step=n)\n",
    "        print(\"SAVED MODEL #{}\".format(n))\n",
    "        \n",
    "    def copy_from(self, other):\n",
    "        mine = [t for t in tf.trainable_variables() \n",
    "                if t.name.startswith(self.scope)]\n",
    "        mine = sorted(mine, key=lambda v: v.name)\n",
    "        others = [t for t in tf.trainable_variables() \n",
    "                if t.name.startswith(other.scope)]\n",
    "        others = sorted(others, key=lambda v: v.name)\n",
    "        \n",
    "        ops = []\n",
    "        for p, q in zip(mine, others):\n",
    "            actual = self.session.run(q)\n",
    "            op = p.assign(actual)\n",
    "            ops.append(op)\n",
    "        \n",
    "        self.session.run(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model,targets_model, experience_replay_buffer, gamma, batch_size):\n",
    "    # sample experiences\n",
    "    samples = random.sample(experience_replay_buffer, batch_size)\n",
    "    states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n",
    "\n",
    "    # calculate targets\n",
    "    next_Qs = target_model.predict(next_states)\n",
    "    next_Q = np.amax(next_Qs, axis=1)\n",
    "    targets = rewards + np.invert(dones).astype(np.float32) * gamma * next_Q\n",
    "\n",
    "#     print(states.shape)\n",
    "#     print(actions.shape)\n",
    "#     print(targets.shape)\n",
    "    \n",
    "    # Update model\n",
    "    loss = model.update(states, actions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "# hyperparameters\n",
    "gamma = 0.99\n",
    "batch_sz = 32\n",
    "num_episodes = 500\n",
    "total_t = 0\n",
    "experience_replay_buffer = []\n",
    "episode_rewards = np.zeros(num_episodes)\n",
    "last_100_avgs = []\n",
    "\n",
    "# epsilon for Epsilon Greedy Algorithm\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_change = (epsilon - epsilon_min) / 500000\n",
    "\n",
    "# Create Atari Environment\n",
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "no saved model to load. starting new session\n",
      "Filling experience replay buffer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing and learning ...\n",
      "Copied model parameters to target network. total_t = 0, period = 10000\n",
      "Episode: 0 Duration: 0:00:18.374611 Num steps: 183 Reward: 0.0 Training time per step: 0.094 Avg Reward (Last 100): 0.000 Epsilon: 1.000\n",
      "SAVED MODEL #0\n",
      "Episode: 1 Duration: 0:00:21.774477 Num steps: 220 Reward: 1.0 Training time per step: 0.096 Avg Reward (Last 100): 0.500 Epsilon: 0.999\n",
      "Episode: 2 Duration: 0:00:28.082412 Num steps: 298 Reward: 2.0 Training time per step: 0.092 Avg Reward (Last 100): 1.000 Epsilon: 0.999\n",
      "Episode: 3 Duration: 0:00:20.463376 Num steps: 210 Reward: 1.0 Training time per step: 0.095 Avg Reward (Last 100): 1.000 Epsilon: 0.998\n",
      "Episode: 4 Duration: 0:00:24.152785 Num steps: 252 Reward: 2.0 Training time per step: 0.094 Avg Reward (Last 100): 1.200 Epsilon: 0.998\n",
      "Episode: 5 Duration: 0:00:20.457821 Num steps: 220 Reward: 1.0 Training time per step: 0.091 Avg Reward (Last 100): 1.167 Epsilon: 0.998\n",
      "Episode: 6 Duration: 0:00:20.866521 Num steps: 220 Reward: 1.0 Training time per step: 0.093 Avg Reward (Last 100): 1.143 Epsilon: 0.997\n",
      "Episode: 7 Duration: 0:00:17.027685 Num steps: 186 Reward: 0.0 Training time per step: 0.089 Avg Reward (Last 100): 1.000 Epsilon: 0.997\n",
      "Episode: 8 Duration: 0:00:16.885342 Num steps: 182 Reward: 0.0 Training time per step: 0.090 Avg Reward (Last 100): 0.889 Epsilon: 0.996\n",
      "Episode: 9 Duration: 0:00:39.407975 Num steps: 387 Reward: 4.0 Training time per step: 0.099 Avg Reward (Last 100): 1.200 Epsilon: 0.996\n",
      "Episode: 10 Duration: 0:00:29.051255 Num steps: 301 Reward: 2.0 Training time per step: 0.094 Avg Reward (Last 100): 1.273 Epsilon: 0.995\n",
      "Episode: 11 Duration: 0:00:26.266023 Num steps: 272 Reward: 2.0 Training time per step: 0.094 Avg Reward (Last 100): 1.333 Epsilon: 0.995\n"
     ]
    }
   ],
   "source": [
    "# Create original and target Networks\n",
    "model = DQN(K=K, scope=\"model\")\n",
    "target_model = DQN(K=K, scope=\"target_model\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.set_session(sess)\n",
    "    target_model.set_session(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model.load()\n",
    "    \n",
    "    print(\"Filling experience replay buffer ...\")\n",
    "    obs = env.reset()\n",
    "    obs_small = preprocess(obs)\n",
    "    state = np.stack([obs_small] * 4, axis = 0)\n",
    "    \n",
    "    # Fill experience replay buffer\n",
    "    for i in range(MIN_EXPERIENCES):\n",
    "        action = np.random.randint(0,K)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        next_state = update_state(state, obs)\n",
    "        \n",
    "        experience_replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            obs_small = preprocess(obs)\n",
    "            state = np.stack([obs_small] * 4, axis = 0)\n",
    "        \n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    print(\"Playing and learning ...\")\n",
    "    # Play a number of episodes and learn\n",
    "    for i in range(num_episodes):\n",
    "        t0 = datetime.now()\n",
    "        \n",
    "        # Reset the environment\n",
    "        obs = env.reset()\n",
    "        obs_small = preprocess(obs)\n",
    "        state = np.stack([obs_small]*4,axis=0)\n",
    "        assert (state.shape == (4,80,80))\n",
    "        loss = None\n",
    "        \n",
    "        total_time_training = 0\n",
    "        num_steps_in_episode = 0\n",
    "        episode_reward = 0\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            # Update target network\n",
    "            if total_t % TARGET_UPDATE_PERIOD == 0:\n",
    "                target_model.copy_from(model)\n",
    "                print(\"Copied model parameters to target network. total_t = %s, period = %s\"\n",
    "                     % (total_t, TARGET_UPDATE_PERIOD))\n",
    "            \n",
    "            # Take action\n",
    "            action = model.sample_action(state, epsilon)\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            obs_small = preprocess(obs)\n",
    "            next_state = np.append(state[1:], np.expand_dims(obs_small, 0), axis=0)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Remove oldest experience if replay buffer is full\n",
    "            if len(experience_replay_buffer) == MAX_EXPERIENCES:\n",
    "                experience_replay_buffer.pop(0)\n",
    "            # Save the recent experience\n",
    "            experience_replay_buffer.append((state,action,reward,next_state,done))\n",
    "            \n",
    "            # Train the model and keep measure of time\n",
    "            t0_2 = datetime.now()\n",
    "            loss = learn(model, target_model, experience_replay_buffer, gamma, batch_sz)\n",
    "            dt = datetime.now() - t0_2\n",
    "            \n",
    "            total_time_training += dt.total_seconds()\n",
    "            num_steps_in_episode += 1\n",
    "            \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "            epsilon = max(epsilon - epsilon_change, epsilon_min)\n",
    "        \n",
    "        duration = datetime.now() - t0\n",
    "        \n",
    "        episode_rewards[i] = episode_reward\n",
    "        time_per_step = total_time_training / num_steps_in_episode\n",
    "        \n",
    "        last_100_avg = episode_rewards[max(0,i-100):i+1].mean()\n",
    "        last_100_avgs.append(last_100_avg)\n",
    "        print(\"Episode:\", i, \"Duration:\",duration, \"Num steps:\", num_steps_in_episode,\n",
    "              \"Reward:\", episode_reward, \"Training time per step:\", \"%.3f\" % time_per_step,\n",
    "              \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg, \"Epsilon:\", \"%.3f\" % epsilon\n",
    "        )\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            model.save(i)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    # Plots\n",
    "    plt.plot(last_100_avgs)\n",
    "    plt.xlabel('episodes')\n",
    "    plt.ylabel('Average Rewards')\n",
    "    plt.show()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
